# GPT-Lab Configuration File

# Basic Information
model_name: "ModdedGPT"  # Name of the model for experiment tracking and logging

# Data Configuration
data:
  # Input data patterns - supports glob patterns
  train_files: "data/fineweb*_train_*.bin"  # Pattern for training data files
  val_files: "data/fineweb*_val_*.bin"      # Pattern for validation data files
  
  # Sequence lengths
  train_seq_len: 8192    # Training sequence length (must be multiple of 128 for FlexAttention)
  val_seq_len: 16384     # Validation sequence length (must be multiple of 128 for FlexAttention)

# Tokenizer Configuration
tokenizer:
  path: "gpt4regex_v50256_n1000000000.pkl"  # Path to tokenizer file relative to data/ directory
  vocab_size: 50257                         # Total vocabulary size (tokenizer vocab size + special tokens)

# Model Architecture
model:
  num_layers: 12         # Number of transformer blocks (must be even for skip connections)
  num_heads: 6           # Number of attention heads
  model_dim: 384         # Size of model embedding vectors
  head_dim: null         # Size of attention heads (if null, defaults to model_dim // num_heads)
  mlp_ratio: 4           # MLP hidden dimension is model_dim * mlp_ratio
  num_val_emb: 2         # Number of value embeddings used at initial and final layers

# Training Configuration
training:
  # Optimization parameters
  val_steps: 10          # Number of steps to run validation for
  train_steps: 20000     # Number of training steps to run
  grad_acc_steps: 1      # Number of gradient accumulation steps per training step
  cooldown_frac: 0.4     # Fraction of training spent cooling down the learning rate
  
  # Optimizer settings (not directly configurable via CLI in original code)
  optimizers:
    adam:
      head_params_lr: 0.22
      embed_params_lr: 0.6
      scalar_params_lr: 0.04
      betas: [0.8, 0.95]
      eps: 1e-10
    muon:
      lr: 0.05
      momentum: 0.95
      nesterov: true
      ns_steps: 5

# Memory Optimization
optimization:
  use_fp8: false        # Experimental FP8 support for matrix multiplication (may improve performance on H100s)

# Evaluation and Logging
evaluation:
  val_loss_every: 100   # How often to evaluate validation loss (0 for only at the end)
  save_model: false     # Whether to save model checkpoints
  
  # Generation parameters for sample outputs
  generation:
    max_new_tokens: 100
    temperature: 0.8
    top_k: 200

# Reproducibility
reproducibility:
  seed: null            # Random seed for initialization (null for random seed)

# Hardware Configuration (automatically determined at runtime)
hardware:
  # These are detected automatically, don't need to be specified
  distributed: auto     # Whether to use distributed training (auto-detected)
